---
id: 00-glossary
title: Glossary
sidebar_position: 0
---

# Glossary

This section provides definitions for key terms and concepts used throughout the textbook, serving as a reference for readers to understand the specialized vocabulary of humanoid robotics and physical AI.

## A

**Actuator**: A component that converts control signals into physical movement. In humanoid robots, actuators control joint motion and are typically servomotors, hydraulic systems, or pneumatic systems.

**AI (Artificial Intelligence)**: The simulation of human intelligence processes by machines, especially computer systems. In robotics, AI enables perception, decision-making, and learning capabilities.

**Artificial Neural Network (ANN)**: A computing system inspired by the biological neural networks that constitute animal brains. Such systems learn to perform tasks by considering examples, generally without being programmed with task-specific rules.

## B

**Balance Control**: The ability of a humanoid robot to maintain its center of mass within its support polygon, preventing falls. Critical for walking and standing stability.

**Behavior Tree**: A hierarchical structure used in robotics and AI to organize and execute complex behaviors. It provides a way to structure decision-making and action execution in autonomous systems.

**Biomechanics**: The study of the structure, function, and motion of the mechanical aspects of biological systems. In humanoid robotics, it informs the design of movement patterns and joint configurations.

## C

**Center of Mass (CoM)**: The point in a body where the total mass of the body may be thought to be concentrated. Critical for balance and stability in humanoid robots.

**Cloud Robotics**: A field of robotics that attempts to invoke cloud technologies for robotics. It provides robots with enormous computational, storage, and learning capabilities by using the cloud.

**Collision Avoidance**: Systems and algorithms that detect potential collisions and take action to prevent them. Essential for safe robot operation around humans and objects.

**Computer Vision**: A field of artificial intelligence that trains computers to interpret and understand the visual world. Cameras capture and identify digital images and videos, which are then processed using algorithms.

## D

**Deep Learning**: A subset of machine learning based on artificial neural networks with representation learning. It can be supervised, semi-supervised or unsupervised.

**Degrees of Freedom (DOF)**: The number of independent movements a mechanical system can make. For humanoid robots, each joint typically represents one or more degrees of freedom.

**Dexterity**: The ability of a robot to perform precise manipulation tasks, typically referring to the hand and arm system's capability to handle objects with precision.

**Digital Twin**: A virtual representation of a physical object or system that spans its lifecycle, is updated with real-time data, and uses simulation, machine learning and reasoning to help decision-making.

**Domain Randomization**: A technique in robotics and computer vision where synthetic training data is generated with randomized visual properties to improve model robustness when deployed in the real world.

## E

**Embodied AI**: Artificial intelligence that is situated in a physical body and interacts with the world through sensors and actuators. It emphasizes the role of physical interaction in intelligence.

**End Effector**: The device at the end of a robotic arm designed to interact with the environment. For humanoid robots, this typically refers to the hand or gripper.

**Edge Computing**: A distributed computing paradigm that brings computation and data storage closer to the location where it is needed, to improve response times and save bandwidth.

## F

**Forward Kinematics**: The use of joint parameters to compute the configuration of the kinematic chain. It determines the position and orientation of the end-effector given the joint angles.

**Fused Deposition Modeling (FDM)**: A 3D printing technology that uses a thermoplastic filament, which is heated to its melting point and then extruded, layer by layer, to create an object.

## G

**Gazebo**: A robot simulation environment that offers the ability to accurately and efficiently simulate populations of robots in complex indoor and outdoor environments.

**General Artificial Intelligence (GAI)**: Hypothetical AI that matches or exceeds human intelligence across the full range of cognitive tasks. Also known as Strong AI or Full AI.

**Gimbal**: A pivoted support that allows an object to rotate about an axis. In robotics, gimbals are used to maintain the orientation of cameras or sensors.

**GPU (Graphics Processing Unit)**: A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.

## H

**Haptic Feedback**: Technology that recreates the sense of touch by applying forces, vibrations, or motions to the user. In humanoid robots, it provides tactile feedback for manipulation tasks.

**Human-Robot Interaction (HRI)**: A field of study focusing on the design, development, and evaluation of robotic systems for human use, emphasizing the interface between humans and robots.

**Humanoid Robot**: A robot with physical features resembling the human body, typically having a head, torso, two arms, and two legs, designed to interact with human environments.

## I

**Inverse Kinematics**: The mathematical process of calculating the variable joint parameters needed to place the end of a kinematic chain, such as a robot hand or leg, in a given position and orientation.

**Isaac Sim**: NVIDIA's next-generation simulation application based on Pixar's Universal Scene Description and NVIDIA Omniverse. It provides a flexible, extensible platform for developing and testing AI-based robotics applications.

**Inertial Measurement Unit (IMU)**: An electronic device that measures and reports a body's specific force, angular rate, and sometimes the magnetic field surrounding the body, using a combination of accelerometers, gyroscopes, and magnetometers.

## J

**Joint Space**: The space defined by the joint angles of a robot. It represents the configuration of the robot in terms of its joint parameters rather than Cartesian space.

## K

**Kinematics**: The study of motion without considering the forces that cause the motion. In robotics, it involves the relationship between joint positions and end-effector positions.

**Kinetic Chain**: A system of rigid bodies connected by joints used to describe the motion of mechanical systems. In humanoid robots, it describes how limbs are connected and move.

## L

**Latency Trap**: A common pitfall in humanoid robotics development where excessive processing delays cause systems to become non-responsive or unstable, particularly critical for real-time control systems.

**LIDAR (Light Detection and Ranging)**: A remote sensing method that uses light in the form of a pulsed laser to measure distances to the Earth. Commonly used in robotics for mapping and navigation.

**Machine Learning**: A method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.

**Manipulation**: The ability of a robot to purposefully control objects in its environment using its end effectors, typically hands or grippers.

**Middleware**: Software that provides common services and capabilities to applications beyond what's offered by the operating system. In robotics, ROS is a common middleware platform.

## M

**Manipulator**: A robot arm or similar device used to manipulate objects in the environment. In humanoid robots, this typically refers to the arms and hands.

**Model Predictive Control (MPC)**: An advanced method of process control that uses a model of the system to predict future behavior and optimize control actions.

**Motor Controller**: A device that governs the operation of an electric motor. In robotics, it controls the speed, torque, position, and other parameters of the motor.

## N

**Navigation**: The ability of a robot to move through an environment from one location to another, typically involving mapping, path planning, and obstacle avoidance.

**Neural Network**: A series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates.

**Node**: In ROS (Robot Operating System), a process that performs computation. Nodes are the fundamental building blocks of ROS applications.

## O

**OpenVLA**: Open Vision-Language-Action, a state-of-the-art Vision-Language-Action model that enables robots to follow natural language instructions and perform complex manipulation tasks.

**Omnidirectional Wheels**: Wheels that can move in any direction, allowing robots to move laterally without changing orientation. Commonly used in mobile robot bases.

**Omniverse**: NVIDIA's platform for 3D design collaboration and virtual world simulation, used in Isaac Sim for robotics simulation.

**Operational Space**: The space in which the robot's end-effector operates, typically defined in Cartesian coordinates (position and orientation).

## P

**Path Planning**: The computational process of determining a route for a robot to follow from its current location to a goal location, considering obstacles and constraints.

**Perception**: The ability of a robot to interpret sensory information from its environment, including vision, touch, sound, and other sensory modalities.

**PID Controller**: A control loop feedback mechanism widely used in industrial control systems. It continuously calculates an error value as the difference between a desired setpoint and a measured process variable.

**Point Cloud**: A set of data points in space, representing the external surface of an object. Commonly generated by 3D scanners and LIDAR systems.

**Pose**: The position and orientation of a robot or object in space, typically described by translation and rotation parameters.

## Q

**QDD (Quasi-Direct Drive)**: A type of actuator design that provides high torque density with low backlash, suitable for high-performance robotic applications.

**Quaternion**: A mathematical concept used to represent rotations and orientations in 3D space, avoiding the gimbal lock problem associated with Euler angles.

## R

**Reinforcement Learning**: A type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward.

**Robot Operating System (ROS)**: A flexible framework for writing robot software. It's not an operating system but a collection of tools, libraries, and conventions for creating robot applications.

**ROS 2**: The second generation of the Robot Operating System, providing improved performance, security, and real-time capabilities.

**ROS Package**: A modular unit of organization in ROS containing libraries, executables, scripts, or other resources used for robot software development.

**ROS Node**: A process that performs computation in a ROS system. Nodes use ROS communication primitives to communicate with other nodes.

**ROS Topic**: A named bus over which nodes exchange messages in a ROS system. Topics implement one-to-many publish-subscribe communication.

**ROS Service**: A synchronous communication pattern in ROS where a client sends a request and waits for a response from a server.

**ROS Action**: An asynchronous communication pattern in ROS for long-running tasks that may take time to complete.

**Real-time Control**: Control systems that must respond to inputs within strict time constraints. Critical for safety and stability in humanoid robots.

**RGB-D Camera**: A camera that captures both color (RGB) and depth (D) information, providing 3D spatial information for robotic perception.

## S

**Sensor Fusion**: The process of combining data from multiple sensors to achieve improved accuracy and reliability compared to using a single sensor.

**Sim-to-Real Transfer**: The process of transferring skills or behaviors learned in simulation to real-world robotic systems.

**SLAM (Simultaneous Localization and Mapping)**: The computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it.

**Speech Recognition**: The ability of a machine or program to identify words and phrases in spoken language and convert them to a machine-readable format.

**State Machine**: A computational model used to design computer programs and logic circuits. In robotics, it's used to define the different operational states of a robot.

**Stereo Vision**: A method of determining distance by comparing two images taken from slightly different positions, mimicking human binocular vision.

**Stereolithography (SLA)**: A 3D printing process that uses photochemical processes to create objects layer by layer from a liquid resin.

## T

**Torque**: The rotational equivalent of linear force. In robotics, it represents the force that causes rotation around a joint axis.

**Trajectory Planning**: The process of determining the path a robot should follow over time, including position, velocity, and acceleration profiles.

**Transformer Model**: A type of deep learning model that uses attention mechanisms to process sequential data, widely used in natural language processing and computer vision.

**Trotting**: A dynamic gait in legged robots where diagonal pairs of legs move together, providing stable forward motion.

## U

**Universal Scene Description (USD)**: Pixar's 3D computer graphics interchange format that enables robust interchange of 3D graphics data between graphics applications.

**Unity**: A cross-platform game engine used for creating 3D and 2D simulations, including robotics applications and digital twins.

## V

**VLA (Vision-Language-Action)**: A type of AI model that processes visual and language inputs to generate robotic actions, enabling natural interaction with robots.

**Vision System**: The combination of cameras, sensors, and software algorithms that allow a robot to perceive and interpret visual information from its environment.

**Virtual Reality (VR)**: A simulated experience that can be similar to or completely different from the real world, used for robot teleoperation and training.

## W

**Walking Gait**: The pattern of leg movements used by bipedal robots to achieve locomotion, including static and dynamic walking patterns.

**Wheelchair Platform**: A mobile robot base using wheels for locomotion, often used as a base for humanoid upper bodies.

**Whisper**: OpenAI's automatic speech recognition system that can transcribe speech to text with high accuracy.

## X

**Xenial**: A codename for Ubuntu 16.04 LTS, a version of the Ubuntu Linux distribution used in some robotics applications.

## Y

**Yaw**: The rotation of a robot around its vertical axis, changing its heading direction.

## Z

**Zero Moment Point (ZMP)**: A concept used in robotics and biomechanics to propose a stability criterion for legged robots, representing the point where the sum of all moments caused by external forces equals zero.

:::tip
Use this glossary as a reference throughout your robotics journey. Many terms have specific meanings in the context of robotics that may differ from their general usage.
:::

:::info
This glossary covers fundamental concepts in humanoid robotics, AI, and related fields. As the field evolves, new terms will emerge and definitions may be refined.
:::